"""
Example script for 1D non-linear system with visualization using Filterformer.

This trains the Filterformer on sequences generated by a 1D nonlinear system
and visualizes the ground truth state vs the model's estimated state over time.
"""

import argparse

import torch
from torch import nn
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np

from models import Filterformer, particle_filter_sequence
from data.nonlinear_system import (
    NonlinearSystem,
    NonlinearSequenceDataset,
    get_device,
)
from data.nonlinear_system import g_nonlinear, f_nonlinear


def set_seed(seed: int = 42) -> None:
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def train_epoch(
    model, loader, optimizer, device: torch.device, grad_clip: float = 1.0
):
    model.train()
    total_loss = 0.0
    n = 0
    for batch in loader:
        y = batch["y"].to(device)
        x = batch["x"].to(device)
        u = batch.get("u")
        if u is not None:
            u = u.to(device)

        optimizer.zero_grad(set_to_none=True)
        x_hat = model(y, u)
        loss = nn.functional.mse_loss(x_hat, x)
        loss.backward()
        if grad_clip is not None:
            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
        optimizer.step()
        total_loss += loss.item() * y.size(0)
        n += y.size(0)
    return total_loss / max(1, n)


@torch.no_grad()
def eval_epoch(model, loader, device: torch.device):
    model.eval()
    total_loss = 0.0
    n = 0
    for batch in loader:
        y = batch["y"].to(device)
        x = batch["x"].to(device)
        u = batch.get("u")
        if u is not None:
            u = u.to(device)
        x_hat = model(y, u)
        loss = nn.functional.mse_loss(x_hat, x)
        total_loss += loss.item() * y.size(0)
        n += y.size(0)
    return total_loss / max(1, n)


def visualize_1d(
    sys: NonlinearSystem,
    model: nn.Module,
    device: torch.device,
    horizon: int = 100,
):
    model.eval()
    X_true, Y, U = sys.generate(horizon=horizon, batch_size=1, noisy=True)
    X_true = X_true.squeeze(0)
    Y = Y.squeeze(0)
    U = U.squeeze(0) if U is not None else None

    with torch.no_grad():
        yb = Y.unsqueeze(0).to(device)
        if U is not None:
            ub = U.unsqueeze(0).to(device)
        else:
            ub = None
        X_hat = model(yb, ub).squeeze(0)
        # Particle filter estimate for comparison
        X_pf = particle_filter_sequence(
            f_nonlinear,
            g_nonlinear,
            q_std=sys.q,
            r_std=sys.r,
            Y=yb,
            num_particles=1024,
            U=ub,
            n_obs=Y.shape[1],
            x0_mean=torch.zeros(1, 1, device=device),
            x0_std=1.0,
        ).squeeze(0)

    steps = np.arange(horizon)
    x_true = X_true.cpu().numpy().flatten()
    x_hat = X_hat.cpu().numpy().flatten()
    x_pf = X_pf.cpu().numpy().flatten()
    y_np = Y.cpu().numpy()

    plt.figure(figsize=(12, 8))

    # Top: state estimates
    plt.subplot(2, 1, 1)
    plt.plot(steps, x_true, "k-", label="Ground Truth", linewidth=2)
    plt.plot(steps, x_hat, "b--", label="Filterformer", linewidth=2)
    plt.plot(steps, x_pf, "r:", label="Particle Filter", linewidth=2)
    plt.xlabel("Time Step")
    plt.ylabel("State Value")
    plt.title("State Estimation Comparison (1D)")
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Bottom: observations
    plt.subplot(2, 1, 2)
    for i in range(y_np.shape[1]):
        plt.plot(
            steps, y_np[:, i], label=f"Measurement Channel {i+1}", alpha=0.7
        )
    plt.xlabel("Time Step")
    plt.ylabel("Measurement Value")
    plt.title("Noisy Measurements (Observations)")
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()


def analyze_state_measurement_relationship_nl(sys: NonlinearSystem):
    """Analyze relationship between state and nonlinear measurements.

    Mirrors the reporting in example_1d.py using the nonlinear g().
    """
    import numpy as np

    print("Generating sample data for analysis...")

    X_true, Y, U = sys.generate(horizon=20, batch_size=1, noisy=True)
    X_true = X_true.squeeze(0).cpu()
    Y = Y.squeeze(0).cpu()

    print("\nNoise parameters:")
    print(f"  Process noise std: {sys.q:.4f}")
    print(f"  Measurement noise std: {sys.r:.4f}")

    # Expected noiseless measurements via g(x)
    with torch.no_grad():
        Y_expected = g_nonlinear(X_true, n_obs=Y.shape[1]).cpu().numpy()

    X_np = X_true.numpy().flatten()
    Y_np = Y.numpy()

    print("\nSample data (first 10 time steps):")
    header = (
        "Time | True State | Meas 1 (noisy) | Meas 1 (expected) | "
        "Meas 2 (noisy) | Meas 2 (expected)"
    )
    print(header)
    print("-" * len(header))
    for t in range(min(10, len(X_np))):
        print(
            f"{t:4d} | {X_np[t]:10.4f} | "
            f"{Y_np[t,0]:13.4f} | {Y_expected[t,0]:16.4f} | "
            f"{Y_np[t,1]:13.4f} | {Y_expected[t,1]:16.4f}"
        )

    # Correlation between state and each measurement
    corr_1 = np.corrcoef(X_np, Y_np[:, 0])[0, 1]
    corr_2 = np.corrcoef(X_np, Y_np[:, 1])[0, 1]

    print("\nCorrelation analysis:")
    print(f"  State vs Measurement 1: {corr_1:.4f}")
    print(f"  State vs Measurement 2: {corr_2:.4f}")

    # Measurement noise estimate
    noise_1 = Y_np[:, 0] - Y_expected[:, 0]
    noise_2 = Y_np[:, 1] - Y_expected[:, 1]

    print("\nNoise analysis:")
    print(
        f"  Measurement 1 noise std: {np.std(noise_1):.4f} "
        f"(expected: {sys.r:.4f})"
    )
    print(
        f"  Measurement 2 noise std: {np.std(noise_2):.4f} "
        f"(expected: {sys.r:.4f})"
    )

    # SNR
    signal_power_1 = float(np.var(Y_expected[:, 0]))
    noise_power_1 = float(np.var(noise_1))
    snr_1 = (
        10 * np.log10(signal_power_1 / noise_power_1)
        if noise_power_1 > 0
        else float("inf")
    )
    signal_power_2 = float(np.var(Y_expected[:, 1]))
    noise_power_2 = float(np.var(noise_2))
    snr_2 = (
        10 * np.log10(signal_power_2 / noise_power_2)
        if noise_power_2 > 0
        else float("inf")
    )

    print("\nSignal-to-Noise Ratio (SNR):")
    print(f"  Measurement 1 SNR: {snr_1:.2f} dB")
    print(f"  Measurement 2 SNR: {snr_2:.2f} dB")
    print("\nAnalysis complete!")


@torch.no_grad()
def eval_with_particle_filter(
    model: nn.Module,
    loader: DataLoader,
    sys: NonlinearSystem,
    *,
    device: torch.device,
) -> tuple[float, float]:
    """Compare Filterformer vs Particle Filter on the validation set.

    Returns (mse_filterformer, mse_particle_filter) averaged over batches/
    timesteps.
    """
    model.eval()
    total_mse_tr = 0.0
    total_mse_pf = 0.0
    count = 0
    for batch in loader:
        y = batch["y"].to(device)
        x_true = batch["x"].to(device)
        u = batch.get("u")
        if u is not None:
            u = u.to(device)
        # Model
        x_tr = model(y, u)
        # Particle filter
        x0_mean = torch.zeros(y.size(0), x_true.size(-1), device=device)
        x_pf = particle_filter_sequence(
            f_nonlinear,
            g_nonlinear,
            q_std=sys.q,
            r_std=sys.r,
            Y=y,
            num_particles=1024,
            U=u,
            n_obs=y.size(-1),
            x0_mean=x0_mean,
            x0_std=1.0,
        )
        total_mse_tr += nn.functional.mse_loss(
            x_tr, x_true, reduction="sum"
        ).item()
        total_mse_pf += nn.functional.mse_loss(
            x_pf, x_true, reduction="sum"
        ).item()
        count += x_true.numel()
    return total_mse_tr / max(1, count), total_mse_pf / max(1, count)


def main():
    parser = argparse.ArgumentParser(
        description="1D Nonlinear Filtering with Filterformer"
    )
    parser.add_argument(
        "--horizon", type=int, default=64, help="Sequence length"
    )
    parser.add_argument(
        "--train_traj", type=int, default=4000, help="# training sequences"
    )
    parser.add_argument(
        "--val_traj", type=int, default=800, help="# validation sequences"
    )
    parser.add_argument(
        "--epochs", type=int, default=15, help="Max epochs"
    )
    parser.add_argument(
        "--batch_size", type=int, default=64, help="Batch size"
    )
    parser.add_argument(
        "--noise_std", type=float, default=0.10, help="Process/Meas noise std"
    )
    parser.add_argument("--lr", type=float, default=2e-3, help="Learning rate")
    parser.add_argument(
        "--patience", type=int, default=3, help="Early stopping patience"
    )
    parser.add_argument(
        "--use_pos_encoding",
        action="store_true",
        help="Use positional encoding",
    )
    parser.add_argument(
        "--layers", type=int, default=4, help="# Transformer layers"
    )
    parser.add_argument(
        "--d_model", type=int, default=128, help="Model dimension"
    )
    parser.add_argument(
        "--n_heads", type=int, default=4, help="# attention heads"
    )
    parser.add_argument("--seed", type=int, default=42, help="Random seed")
    args = parser.parse_args()

    set_seed(args.seed)
    device = get_device()
    print(
        "=== Nonlinear Filtering - Filterformer Example ==="
    )
    print(
        "This example trains Filterformer to approximate a 1D nonlinear "
        "filter and then visualizes the ground truth vs the model estimate."
    )
    print()
    print(f"Using device: {device}")

    # Nonlinear 1D system with 2D observations
    n_state, n_obs, n_ctrl = 1, 2, 0
    sys = NonlinearSystem(
        n_state=n_state,
        n_obs=n_obs,
        n_ctrl=n_ctrl,
        process_noise_std=args.noise_std,
        meas_noise_std=args.noise_std,
        device=device,
    )

    # Datasets
    train_ds = NonlinearSequenceDataset(
        sys,
        num_traj=args.train_traj,
        horizon=args.horizon,
        random_controls=False,
        noisy=True,
    )
    val_ds = NonlinearSequenceDataset(
        sys,
        num_traj=args.val_traj,
        horizon=args.horizon,
        random_controls=False,
        noisy=True,
    )

    pin = device.type == "cuda"
    train_loader = DataLoader(
        train_ds, batch_size=args.batch_size, shuffle=True, pin_memory=pin
    )
    val_loader = DataLoader(
        val_ds, batch_size=args.batch_size, shuffle=False, pin_memory=pin
    )

    # Model
    cfg_like = type("Cfg", (), {})()
    cfg_like.d_model = args.d_model
    cfg_like.n_heads = args.n_heads
    cfg_like.dropout = 0.1
    cfg_like.max_len = max(1024, args.horizon)
    cfg_like.use_positional_encoding = args.use_pos_encoding

    model = Filterformer(
        n_obs=n_obs,
        n_state=n_state,
        n_ctrl=n_ctrl,
        cfg=cfg_like,
        num_layers=args.layers,
        output_covariance=False,
    ).to(device)

    print()
    print("Configuration:")
    print(f"  State dimension: {n_state}")
    print(f"  Observation dimension: {n_obs}")
    print(f"  Sequence length: {args.horizon}")
    print(f"  Training trajectories: {args.train_traj}")
    print(f"  Validation trajectories: {args.val_traj}")
    print(f"  Max epochs: {args.epochs}")
    print(f"  Noise std: {args.noise_std}")
    print(f"  Layers: {args.layers}")
    print(f"  d_model: {args.d_model}")
    print(f"  n_heads: {args.n_heads}")
    print(f"  Positional encoding: {args.use_pos_encoding}")
    print(
        f"  Model parameters: "
        f"{sum(p.numel() for p in model.parameters()):,}"
    )
    print()
    print("Nonlinear system (1D state, 2D observation):")
    print("  State update:")
    print(
        "    x_{t+1} = 0.85 x_t + tanh(x_t) + 0.05*tanh(x_t^2) "
    )
    print("              + 0.01*C x_t + B u_t + w_t")
    print("    (C has entries 0.02; u_t omitted when n_ctrl=0)")
    print("  Observation:")
    print("    y_t = W [x_t, sin(x_t), cos(x_t)] + v_t")

    optimizer = torch.optim.AdamW(
        model.parameters(), lr=args.lr, weight_decay=1e-2
    )

    # Training loop with early stopping (match example_1d.py style)
    best_val = float("inf")
    patience_counter = 0
    print("=" * 80)
    print("TRAINING PROGRESS")
    print("=" * 80)
    print(
        f"{'Epoch':<6} | {'Train MSE':<10} | {'Val MSE':<10} | "
        f"{'Best Val MSE':<12} | {'Status':<15}"
    )
    print("-" * 80)
    for epoch in range(1, args.epochs + 1):
        tr = train_epoch(model, train_loader, optimizer, device)
        val = eval_epoch(model, val_loader, device)
        is_best = val < best_val
        if is_best:
            best_val = val
            patience_counter = 0
            status = "New best!"
        else:
            patience_counter += 1
            status = f"Patience: {patience_counter}/{args.patience}"
        print(
            f"{epoch:6d} | {tr:10.6f} | {val:10.6f} | {best_val:12.6f} | "
            f"{status:<15}"
        )
        if patience_counter >= args.patience:
            print("Early stopping (patience exceeded)")
            break

    # Final evaluation (match example_1d.py style)
    print("-" * 80)
    print("FINAL EVALUATION")
    print("-" * 80)
    tr_mse, pf_mse = eval_with_particle_filter(
        model, val_loader, sys, device=device
    )
    print("Validation MSE vs Ground Truth:")
    print(f"  Filterformer: {tr_mse:.6f}")
    print(f"  Particle Filter: {pf_mse:.6f}")
    if pf_mse > 0:
        print(f"  Performance ratio: {tr_mse/pf_mse:.3f}x")
    else:
        print("  Performance ratio: inf (PF MSE is zero)")

    # Visualization on a fresh trajectory
    visualize_1d(sys, model, device, horizon=max(100, args.horizon))
    print("\nAnalyzing state-measurement relationship...")
    analyze_state_measurement_relationship_nl(sys)
    print()
    print("Training completed!")
    print(f"Best validation loss: {best_val:.6f}")
    print("Visualization shows:")
    print("- Ground truth state (black line)")
    print("- Filterformer prediction (blue dashed line)")


if __name__ == "__main__":
    main()

