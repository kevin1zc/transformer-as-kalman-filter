"""
Example script for 1D non-linear system with visualization using Filterformer.

This trains the Filterformer on sequences generated by a 1D nonlinear system
and visualizes the ground truth state vs the model's estimated state over time.
"""

import argparse

import torch
from torch import nn
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm

from models import Filterformer, particle_filter_with_system
from data.nonlinear_system import (
    NonlinearSystem,
    NonlinearSequenceDataset,
    get_device,
)


def set_seed(seed: int = 42) -> None:
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def train_epoch(
    model, loader, optimizer, device: torch.device, grad_clip: float = 1.0
):
    model.train()
    total_loss = 0.0
    n = 0
    pbar = tqdm(loader, desc="Training", leave=False, position=1)
    for batch in pbar:
        y = batch["y"].to(device)
        x = batch["x"].to(device)
        u = batch.get("u")
        if u is not None:
            u = u.to(device)

        optimizer.zero_grad(set_to_none=True)
        x_hat = model(y, u)
        loss = nn.functional.mse_loss(x_hat, x)
        loss.backward()
        if grad_clip is not None:
            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
        optimizer.step()
        total_loss += loss.item() * y.size(0)
        n += y.size(0)
        
        # Update progress bar
        pbar.set_postfix({'loss': f'{loss.item():.6f}'})
    return total_loss / max(1, n)


@torch.no_grad()
def eval_epoch(model, loader, device: torch.device):
    model.eval()
    total_loss = 0.0
    n = 0
    pbar = tqdm(loader, desc="Validation", leave=False, position=1)
    for batch in pbar:
        y = batch["y"].to(device)
        x = batch["x"].to(device)
        u = batch.get("u")
        if u is not None:
            u = u.to(device)
        x_hat = model(y, u)
        loss = nn.functional.mse_loss(x_hat, x)
        total_loss += loss.item() * y.size(0)
        n += y.size(0)
        
        # Update progress bar
        pbar.set_postfix({'loss': f'{loss.item():.6f}'})
    return total_loss / max(1, n)


def visualize_1d(
    sys: NonlinearSystem,
    model: nn.Module,
    device: torch.device,
    horizon: int = 100,
):
    model.eval()
    X_true, Y, U = sys.generate(horizon=horizon, batch_size=1, noisy=True)
    X_true = X_true.squeeze(0)
    Y = Y.squeeze(0)
    U = U.squeeze(0) if U is not None else None

    with torch.no_grad():
        yb = Y.unsqueeze(0).to(device)
        if U is not None:
            ub = U.unsqueeze(0).to(device)
        else:
            ub = None
        X_hat = model(yb, ub).squeeze(0)
        # Particle filter estimate for comparison
        X_pf = particle_filter_with_system(
            sys,
            Y=yb,
            num_particles=1024,
            U=ub,
            x0_mean=torch.zeros(1, 1, device=device),
            x0_std=1.0,
        ).squeeze(0)

    steps = np.arange(horizon)
    x_true = X_true.cpu().numpy().flatten()
    x_hat = X_hat.cpu().numpy().flatten()
    x_pf = X_pf.cpu().numpy().flatten()
    y_np = Y.cpu().numpy()

    plt.figure(figsize=(12, 8))

    # Top: state estimates
    plt.subplot(2, 1, 1)
    plt.plot(steps, x_true, "k-", label="Ground Truth", linewidth=2)
    plt.plot(steps, x_hat, "b--", label="Filterformer", linewidth=2)
    plt.plot(steps, x_pf, "r:", label="Particle Filter", linewidth=2)
    plt.xlabel("Time Step")
    plt.ylabel("State Value")
    plt.title("State Estimation Comparison (1D)")
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Bottom: observations
    plt.subplot(2, 1, 2)
    for i in range(y_np.shape[1]):
        plt.plot(
            steps, y_np[:, i], label=f"Measurement Channel {i+1}", alpha=0.7
        )
    plt.xlabel("Time Step")
    plt.ylabel("Measurement Value")
    plt.title("Noisy Measurements (Observations)")
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()


def analyze_state_measurement_relationship_nl(sys: NonlinearSystem):
    """Analyze relationship between state and nonlinear measurements.

    Mirrors the reporting in example_1d.py using the nonlinear g().
    """
    import numpy as np

    print("\n" + "="*80)
    print("SYSTEM ANALYSIS")
    print("="*80)
    
    # Display the actual random system parameters
    sys_info = sys.get_system_info()
    print("Random Nonlinear System Parameters:")
    print("  Transition function f(x,u):")
    f_params = sys_info['f_params']
    print(f"    Linear weight: {f_params['linear_weight']:.4f}")
    print(f"    Tanh weight: {f_params['tanh_weight']:.4f}")
    print(f"    Sin weight: {f_params['sin_weight']:.4f}")
    print(f"    Cos weight: {f_params['cos_weight']:.4f}")
    print(f"    Quadratic weight: {f_params['quad_weight']:.4f}")
    print(f"    Cubic weight: {f_params['cubic_weight']:.4f}")
    print(f"    Mix weight: {f_params['mix_weight']:.4f}")
    print(f"    Control weight: {f_params['control_weight']:.4f}")
    
    print("  Observation function g(x):")
    g_params = sys_info['g_params']
    print(f"    Linear weight: {g_params['linear_weight']:.4f}")
    print(f"    Sin weight: {g_params['sin_weight']:.4f}")
    print(f"    Cos weight: {g_params['cos_weight']:.4f}")
    print(f"    Tanh weight: {g_params['tanh_weight']:.4f}")
    print(f"    Square weight: {g_params['square_weight']:.4f}")

    print("\nGenerating fresh sample data for analysis...")

    X_true, Y, U = sys.generate(horizon=20, batch_size=1, noisy=True)
    X_true = X_true.squeeze(0).cpu()
    Y = Y.squeeze(0).cpu()

    print("\nNoise parameters:")
    print(f"  Process noise std: {sys.q:.4f}")
    print(f"  Measurement noise std: {sys.r:.4f}")

    # Expected noiseless measurements via g(x)
    with torch.no_grad():
        Y_expected = sys.system_generator.g(X_true).cpu().numpy()

    X_np = X_true.numpy().flatten()
    Y_np = Y.numpy()

    print("\nSample data (first 10 time steps):")
    header = (
        "Time | True State | Meas 1 (noisy) | Meas 1 (expected) | "
        "Meas 2 (noisy) | Meas 2 (expected)"
    )
    print(header)
    print("-" * len(header))
    for t in range(min(10, len(X_np))):
        print(
            f"{t:4d} | {X_np[t]:10.4f} | "
            f"{Y_np[t,0]:13.4f} | {Y_expected[t,0]:16.4f} | "
            f"{Y_np[t,1]:13.4f} | {Y_expected[t,1]:16.4f}"
        )

    # Correlation between state and each measurement
    corr_1 = np.corrcoef(X_np, Y_np[:, 0])[0, 1]
    corr_2 = np.corrcoef(X_np, Y_np[:, 1])[0, 1]

    print("\nCorrelation analysis:")
    print(f"  State vs Measurement 1: {corr_1:.4f}")
    print(f"  State vs Measurement 2: {corr_2:.4f}")

    # Measurement noise estimate
    noise_1 = Y_np[:, 0] - Y_expected[:, 0]
    noise_2 = Y_np[:, 1] - Y_expected[:, 1]

    print("\nNoise analysis:")
    print(
        f"  Measurement 1 noise std: {np.std(noise_1):.4f} "
        f"(expected: {sys.r:.4f})"
    )
    print(
        f"  Measurement 2 noise std: {np.std(noise_2):.4f} "
        f"(expected: {sys.r:.4f})"
    )

    # SNR
    signal_power_1 = float(np.var(Y_expected[:, 0]))
    noise_power_1 = float(np.var(noise_1))
    snr_1 = (
        10 * np.log10(signal_power_1 / noise_power_1)
        if noise_power_1 > 0
        else float("inf")
    )
    signal_power_2 = float(np.var(Y_expected[:, 1]))
    noise_power_2 = float(np.var(noise_2))
    snr_2 = (
        10 * np.log10(signal_power_2 / noise_power_2)
        if noise_power_2 > 0
        else float("inf")
    )

    print("\nSignal-to-Noise Ratio (SNR):")
    print(f"  Measurement 1 SNR: {snr_1:.2f} dB")
    print(f"  Measurement 2 SNR: {snr_2:.2f} dB")
    print("\nAnalysis complete!")


@torch.no_grad()
def eval_with_particle_filter(
    model: nn.Module,
    loader: DataLoader,
    sys: NonlinearSystem,
    *,
    device: torch.device,
) -> tuple[float, float]:
    """Compare Filterformer vs Particle Filter on the validation set.

    Returns (mse_filterformer, mse_particle_filter) averaged over batches/
    timesteps.
    """
    model.eval()
    total_mse_tr = 0.0
    total_mse_pf = 0.0
    count = 0
    pbar = tqdm(loader, desc="Evaluating vs Particle Filter", leave=False, position=1)
    for batch in pbar:
        y = batch["y"].to(device)
        x_true = batch["x"].to(device)
        u = batch.get("u")
        if u is not None:
            u = u.to(device)
        # Model
        x_tr = model(y, u)
        # Particle filter
        x0_mean = torch.zeros(y.size(0), x_true.size(-1), device=device)
        x_pf = particle_filter_with_system(
            sys,
            Y=y,
            num_particles=1024,
            U=u,
            x0_mean=x0_mean,
            x0_std=1.0,
        )
        mse_tr = nn.functional.mse_loss(x_tr, x_true, reduction="sum").item()
        mse_pf = nn.functional.mse_loss(x_pf, x_true, reduction="sum").item()
        total_mse_tr += mse_tr
        total_mse_pf += mse_pf
        count += x_true.numel()
        
        # Update progress bar
        pbar.set_postfix({
            'Filterformer MSE': f'{mse_tr/count:.6f}',
            'Particle Filter MSE': f'{mse_pf/count:.6f}'
        })
    return total_mse_tr / max(1, count), total_mse_pf / max(1, count)


def main():
    parser = argparse.ArgumentParser(
        description="1D Nonlinear Filtering with Filterformer"
    )
    parser.add_argument(
        "--horizon", type=int, default=64, help="Sequence length"
    )
    parser.add_argument(
        "--train_traj", type=int, default=40000, help="# training sequences"
    )
    parser.add_argument(
        "--val_traj", type=int, default=10000, help="# validation sequences"
    )
    parser.add_argument(
        "--epochs", type=int, default=20, help="Max epochs"
    )
    parser.add_argument(
        "--batch_size", type=int, default=64, help="Batch size"
    )
    parser.add_argument(
        "--noise_std", type=float, default=0.05, help="Process/Meas noise std"
    )
    parser.add_argument("--lr", type=float, default=2e-3, help="Learning rate")
    parser.add_argument(
        "--patience", type=int, default=3, help="Early stopping patience"
    )
    parser.add_argument(
        "--layers", type=int, default=2, help="# Transformer layers"
    )
    parser.add_argument(
        "--d_model", type=int, default=128, help="Model dimension"
    )
    parser.add_argument(
        "--n_heads", type=int, default=4, help="# attention heads"
    )
    parser.add_argument("--seed", type=int, default=42, help="Random seed")
    args = parser.parse_args()

    set_seed(args.seed)
    device = get_device()
    print(
        "=== Nonlinear Filtering - Filterformer Example ==="
    )
    print(
        "This example trains Filterformer to approximate a 1D nonlinear "
        "filter and then visualizes the ground truth vs the model estimate."
    )
    print()
    print(f"Using device: {device}")

    # Nonlinear 1D system with 2D observations
    n_state, n_obs, n_ctrl = 1, 2, 0
    sys = NonlinearSystem(
        n_state=n_state,
        n_obs=n_obs,
        n_ctrl=n_ctrl,
        process_noise_std=args.noise_std,
        meas_noise_std=args.noise_std,
        device=device,
    )

    # Datasets
    train_ds = NonlinearSequenceDataset(
        sys,
        num_traj=args.train_traj,
        horizon=args.horizon,
        random_controls=False,
        noisy=True,
    )
    val_ds = NonlinearSequenceDataset(
        sys,
        num_traj=args.val_traj,
        horizon=args.horizon,
        random_controls=False,
        noisy=True,
    )

    pin = device.type == "cuda"
    train_loader = DataLoader(
        train_ds, batch_size=args.batch_size, shuffle=True, pin_memory=pin
    )
    val_loader = DataLoader(
        val_ds, batch_size=args.batch_size, shuffle=False, pin_memory=pin
    )

    # Model
    cfg_like = type("Cfg", (), {})()
    cfg_like.d_model = args.d_model
    cfg_like.n_heads = args.n_heads
    cfg_like.dropout = 0.1
    cfg_like.max_len = max(1024, args.horizon)

    model = Filterformer(
        n_obs=n_obs,
        n_state=n_state,
        n_ctrl=n_ctrl,
        cfg=cfg_like,
        num_layers=args.layers,
        output_covariance=False,
    ).to(device)

    print()
    print("Configuration:")
    print(f"  State dimension: {n_state}")
    print(f"  Observation dimension: {n_obs}")
    print(f"  Sequence length: {args.horizon}")
    print(f"  Training trajectories: {args.train_traj}")
    print(f"  Validation trajectories: {args.val_traj}")
    print(f"  Max epochs: {args.epochs}")
    print(f"  Noise std: {args.noise_std}")
    print(f"  Layers: {args.layers}")
    print(f"  d_model: {args.d_model}")
    print(f"  n_heads: {args.n_heads}")
    print(f"  Positional encoding: True (always enabled)")
    
    # Calculate and display parameter-to-data ratio
    total_params = sum(p.numel() for p in model.parameters())
    total_data_points = args.train_traj * args.horizon
    param_data_ratio = total_data_points / total_params
    
    print(f"  Model parameters: {total_params:,}")
    print(f"  Training data points: {total_data_points:,}")
    print(f"  Data-to-parameter ratio: {param_data_ratio:.1f}x")

    optimizer = torch.optim.AdamW(
        model.parameters(), lr=args.lr, weight_decay=1e-2
    )

    # Training loop with early stopping (match example_1d.py style)
    best_val = float("inf")
    patience_counter = 0
    print("=" * 80)
    print("TRAINING PROGRESS")
    print("=" * 80)
    print(
        f"{'Epoch':<6} | {'Train MSE':<10} | {'Val MSE':<10} | "
        f"{'Best Val MSE':<12} | {'Status':<15}"
    )
    print("-" * 80)
    
    epoch_pbar = tqdm(range(1, args.epochs + 1), desc="Training Epochs", position=0, leave=True)
    for epoch in epoch_pbar:
        tr = train_epoch(model, train_loader, optimizer, device)
        val = eval_epoch(model, val_loader, device)
        is_best = val < best_val
        if is_best:
            best_val = val
            patience_counter = 0
            status = "New best!"
        else:
            patience_counter += 1
            status = f"Patience: {patience_counter}/{args.patience}"
        
        # Update epoch progress bar
        epoch_pbar.set_postfix({
            'Train MSE': f'{tr:.6f}',
            'Val MSE': f'{val:.6f}',
            'Best Val MSE': f'{best_val:.6f}',
            'Status': status
        })
        
        # Print above the progress bar
        tqdm.write(
            f"{epoch:6d} | {tr:10.6f} | {val:10.6f} | {best_val:12.6f} | "
            f"{status:<15}"
        )
        # Don't early stop for the first 5 epochs
        if patience_counter >= args.patience and epoch >= 5:
            tqdm.write("Early stopping (patience exceeded)")
            break

    # Final evaluation (match example_1d.py style)
    print("-" * 80)
    print("FINAL EVALUATION")
    print("-" * 80)
    tr_mse, pf_mse = eval_with_particle_filter(
        model, val_loader, sys, device=device
    )
    print("Validation MSE vs Ground Truth:")
    print(f"  Filterformer: {tr_mse:.6f}")
    print(f"  Particle Filter: {pf_mse:.6f}")
    if pf_mse > 0:
        print(f"  Performance ratio: {tr_mse/pf_mse:.3f}x")
    else:
        print("  Performance ratio: inf (PF MSE is zero)")

    # Visualization on a fresh trajectory
    visualize_1d(sys, model, device, horizon=max(100, args.horizon))
    print("\nAnalyzing state-measurement relationship...")
    analyze_state_measurement_relationship_nl(sys)
    print()
    print("Training completed!")
    print(f"Best validation loss: {best_val:.6f}")
    print("Visualization shows:")
    print("- Ground truth state (black line)")
    print("- Filterformer prediction (blue dashed line)")


if __name__ == "__main__":
    main()

